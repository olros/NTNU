{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=ewRw996uevM&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=18\n",
    "#https://www.youtube.com/watch?v=0bt0SjbS3xc&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import  RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "MODEL_FILE_NAME = \"dqncartpole.h5\"\n",
    "env = gym.make('CartPole-v0')\n",
    "tf.random.set_seed(200)\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU')) > 0\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "tf.test.is_built_with_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space = env.observation_space.shape[0] \n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space= env.action_space.n\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNQLearnCartPoleSolver():\n",
    "    def __init__(self, env,  input_shape, action_shape, episodes, epsilon_decay_rate=0.995, min_epsilon=0.001):\n",
    "        self.input_size = input_shape\n",
    "        self.episodes = episodes\n",
    "        self.env = env\n",
    "        self.action_size = action_shape\n",
    "        self.memory = deque([],maxlen=2000)\n",
    "        self.min_epsilon=min_epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.epsilon = 0.1\n",
    "        self.state_size = input_shape\n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.train_start = 128\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=input_shape, activation='relu', kernel_initializer='he_uniform'))\n",
    "        self.model.add(Dense(action_shape, activation=\"linear\", kernel_initializer='he_uniform'))\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer=RMSprop(\n",
    "            learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    " \n",
    "\n",
    "    def action(self, state):\n",
    "        # print(f\" rand nr {np.random.random()}  eps {self.epsilon}\")\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.state_size]) \n",
    "  \n",
    "    def update_q_func(self,reward, next_state, done):\n",
    "        if done:\n",
    "            return reward\n",
    "        else:\n",
    "            return reward + self.gamma * np.max(next_state)\n",
    "\n",
    "    def update_q_values(self, minibatch, target, target_next ):\n",
    "        for index, (_, action, reward, _, done) in enumerate(minibatch):\n",
    "            target[index][action] = self.update_q_func(reward, target_next[index], done)\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon)\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        for index, (state, _, _, next_state, _ )in enumerate(minibatch):\n",
    "            states[index] = state\n",
    "            next_states[index] = next_state\n",
    "        target = self.model.predict(states)\n",
    "        target_next = self.model.predict(next_states)\n",
    "        self.update_q_values(minibatch, target, target_next)\n",
    "        self.model.fit(np.array(states), np.array(target), batch_size=self.batch_size, verbose=0)\n",
    "        self.update_epsilon()\n",
    "    \n",
    "\n",
    "    def get_reward(self, done, step, reward):\n",
    "        if not done or step == self.env._max_episode_steps-1:\n",
    "                return reward\n",
    "        else:\n",
    "            return -100\n",
    "    \n",
    "            \n",
    "    def train(self):\n",
    "        scores = []\n",
    "        for episode in range(self.episodes):\n",
    "            done = False\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            step = 0\n",
    "            while not done:\n",
    "                action = self.action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                next_state =  self.preprocess_state(next_state)\n",
    "                reward = self.get_reward(done, step, reward)\n",
    "                step +=1\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "            scores.append(step)\n",
    "            print(f\"{scores[episode]}  score for ep {episode+1} epsilon {self.epsilon}\")\n",
    "            if step == 200:\n",
    "                print(f\"Saving trained model as {MODEL_FILE_NAME}\")\n",
    "                self.model.save(MODEL_FILE_NAME)\n",
    "            self.replay()\n",
    "        print('Finished training!')\n",
    "\n",
    "    def test(self):\n",
    "        self.model = load_model(MODEL_FILE_NAME)\n",
    "        state = self.preprocess_state(self.env.reset())\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = np.argmax(self.model.predict(state))\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            state = self.preprocess_state(next_state)\n",
    "            score += 1\n",
    "        print(f\"{score}  score\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69  score for ep 1 epsilon 0.1\n",
      "79  score for ep 2 epsilon 0.1\n",
      "180  score for ep 3 epsilon 0.0995\n",
      "122  score for ep 4 epsilon 0.09900250000000001\n",
      "61  score for ep 5 epsilon 0.0985074875\n",
      "58  score for ep 6 epsilon 0.09801495006250001\n",
      "62  score for ep 7 epsilon 0.09752487531218751\n",
      "124  score for ep 8 epsilon 0.09703725093562657\n",
      "131  score for ep 9 epsilon 0.09655206468094843\n",
      "46  score for ep 10 epsilon 0.09606930435754368\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DQNQLearnCartPoleSolver(env, state_space, action_space, episodes=10)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69  score\n"
     ]
    }
   ],
   "source": [
    "model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "## Model\n",
    "```python\n",
    "self.model = Sequential()\n",
    "self.model.add(Dense(24, input_dim=input_shape, activation='relu',kernel_initializer='he_uniform'))\n",
    "self.model.add(Dense(action_shape, activation=\"linear\", kernel_initializer='he_uniform'))\n",
    "```\n",
    "\n",
    "We start with an input layer of the size of the observation space of the enviorment as we can see at the top of the file it is 4, then comes the  neural network part of the model which are all the dense layers, which creates hidden layers with n nodes. Every Hidden layer is wraped with a relu activation function which simplifies the data in the network, this is done by applying a max function on the value and 0 which leads to only positive values. Every layer also has a kernel initializer set to he uniform which initializes all the weights to non zero values in the different layers. More spesificly it draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor. The last layer has activation linear to shape the output of the model. This model has only one hidden layer because the cartpole problem is a pretty easy problem to solve.\n",
    "\n",
    "## Optimizer\n",
    "```python\n",
    "self.model.compile(loss=\"mse\", optimizer=RMSprop(\n",
    "            learning_rate=0.00025, epsilon=0.01), metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "Since DQN is a RNN the RMSprop optimizer is used, which beats out normal gradient decent and adam optimizers for RNN, read more here: [Optimizers](https://ruder.io/optimizing-gradient-descent/index.html)\n",
    "\n",
    "## Code\n",
    "Most of the logic here is the same as in QLearn-cartpole.ipynb\n",
    "\n",
    "When training we firstly loop for n episodes given to the model on creation. For each episode we reset the env as the init state\n",
    " ```python  \n",
    " self.preprocess_state(self.env.reset())\n",
    "\n",
    " def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, self.state_size]) \n",
    "  \n",
    " ```\n",
    "\n",
    "\n",
    "state is always transformed into and (1, n) array so it can be used as an input to the model. for every episode we loop until the agent has either failed by tiping or won by getting 200 points. we then choose an action, and preform that action.\n",
    "```python\n",
    "action = self.action(state)\n",
    "next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "\n",
    " def action(self, state):\n",
    "    if np.random.random() <= self.epsilon:\n",
    "        return random.randrange(self.action_size)\n",
    "    else:\n",
    "        return np.argmax(self.model.predict(state))\n",
    "```\n",
    "when choosing an action we either explore or exploit this is chosen by generating a random number between 0, 1 and then comparing it to epsilon. if the number is smaller than epsilon we use the model to predict a action (exploit) same as in normal q-learning if the number is lager than epsilon we choose a random action in the env.\n",
    "\n",
    "\n",
    "In normal q-learn we would now update the q table with new q values by Optimizing the q function, but in DQN we instead push the current state, action, reward, next state, and if the agent is done or not. Since we dont update q-values in a DQN we instead use the stored values to \"replay\" the previous attempt and train on the model on the values stored in replay memory which is where we put all the values from each step during the while loop. This is done by firstly picking out a subset of values from the replay memory\n",
    "```python\n",
    "minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
    "```\n",
    "we then use the model to predict q values for the current state and the next state\n",
    "\n",
    "```python\n",
    "target = self.model.predict(states)\n",
    "target_next = self.model.predict(next_states)\n",
    "```\n",
    "we then update the target values which we are going to use for training with the predicted next\n",
    "\n",
    "```python\n",
    "def update_q_func(self,reward, next_state, done):\n",
    "    if done:\n",
    "        return reward\n",
    "    else:\n",
    "        return reward + self.gamma * np.max(next_state)\n",
    "\n",
    "def update_q_values(self, minibatch, target, target_next ):\n",
    "    for index, (_, action, reward, _, done) in enumerate(minibatch):\n",
    "        target[index][action] = self.update_q_func(reward, target_next[index], done)\n",
    "```\n",
    "\n",
    "then we train the model with the the states from the batch and the targets\n",
    "```python\n",
    "self.model.fit(np.array(states), np.array(target), batch_size=self.batch_size, verbose=0)\n",
    "```\n",
    "we then update the epsilon value to reduce it since the model is now better so we should trust it more and exploit more then we explore.\n",
    "\n",
    "```python\n",
    "def update_epsilon(self):\n",
    "    if self.epsilon > self.min_epsilon:\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "```\n",
    "\n",
    "## Side notes\n",
    "With a lower epsilon the DQN seems to preform better faster, this might be because it results in less exploration and more exploitation which might in this case be good.\n",
    "\n",
    "\"I think the problem is with openAI gym CartPole-v0 environment reward structure. The reward is always +1 for each time step. So if pole falls reward is +1 itself. So we need to check and redefine the reward for this case. So in the train function try this:\"\n",
    "\n",
    "```python\n",
    "if not done:\n",
    "    new_q = reward + DISCOUNT * np.max(future_qs_list)\n",
    "else:\n",
    "    # if done assign some negative reward\n",
    "    new_q = -20\n",
    "```\n",
    "[Source](https://ai.stackexchange.com/questions/22986/my-deep-q-learning-network-does-not-learn-for-openai-gyms-cartpole-problem)\n",
    "\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
