{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Q-learning to solve CartPole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Base code taken from: \n",
    "https://github.com/IsaacPatole/CartPole-v0-using-Q-learning-SARSA-and-DNN/blob/master/Qlearning_for_cartpole.py\n",
    "\"\"\"\n",
    "\n",
    "class CartPoleQAgent():\n",
    "    def __init__(self, buckets=(3, 3, 6, 6), \n",
    "                 num_episodes=200, min_lr=0.1, \n",
    "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        # This is the action-value function being initialized to 0's\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "        # [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        \n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Discretizes the state, transforming the new state into a tuple that can be fed to the Q_table\"\"\"\n",
    "        discretized = list()\n",
    "        for i in range(len(state)):\n",
    "            scaling = ((state[i] + abs(self.lower_bounds[i])) \n",
    "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
    "            new_state = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_state = min(self.buckets[i] - 1, max(0, new_state))\n",
    "            discretized.append(new_state)\n",
    "        return tuple(discretized)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Chooses an action to perform, does a random action from time to time because of the epsilon-variable.\"\"\"\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        \"\"\"Updates Q-table using the Q-formula rule.\"\"\"\n",
    "        self.Q_table[state][action] += (self.learning_rate * \n",
    "                                        (reward \n",
    "                                         + self.discount * np.max(self.Q_table[new_state]) \n",
    "                                         - self.Q_table[state][action]))\n",
    "\n",
    "    def get_epsilon(self, episode):\n",
    "        \"\"\"\n",
    "        Gets value for epsilon. It declines as we advance in episodes.\n",
    "        This is to done to ensure that the chance of randomly exploring is high\n",
    "        when we've run a few episodes and goes towards the lowest after a lot of episodes.\n",
    "        The use of max() and min() ensure that epsilon always is between `self.min_epsilon` and 1.0\n",
    "        \"\"\"\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((episode + 1) / self.decay)))\n",
    "\n",
    "    def get_learning_rate(self, episode):\n",
    "        \"\"\"Gets value for learning rate. It declines as we advance in episodes or as we add more episodes.\"\"\"\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((episode + 1) / self.decay)))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains agent making it go through the environment,\n",
    "        choose actions and update values for its Q-table.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "\n",
    "            episode_score = 0\n",
    "            \n",
    "            # Do steps until we're done\n",
    "            while not done:\n",
    "                # Choose action from the state\n",
    "                action = self.choose_action(current_state)\n",
    "                # Do the selected action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                # Update the Q_table\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "\n",
    "                episode_score += reward\n",
    "                \n",
    "            scores.append(episode_score)\n",
    "            print(f\"{scores[e]} score for episode {e+1}\")\n",
    "        print('Finished training!')\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        self.env = gym.wrappers.Monitor(self.env,'cartpole', force=True)\n",
    "        done = False\n",
    "        current_state = self.discretize_state(self.env.reset())\n",
    "        score = 0.0\n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                score += reward\n",
    "                new_state = self.discretize_state(obs)\n",
    "                current_state = new_state\n",
    "            \n",
    "        print(f\"Score: {str(score)}\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent and run\n",
    "The following function trains the CartPoleAgent and then runs one round visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0 score for episode 1\n",
      "24.0 score for episode 2\n",
      "22.0 score for episode 3\n",
      "37.0 score for episode 4\n",
      "27.0 score for episode 5\n",
      "15.0 score for episode 6\n",
      "18.0 score for episode 7\n",
      "9.0 score for episode 8\n",
      "40.0 score for episode 9\n",
      "51.0 score for episode 10\n",
      "16.0 score for episode 11\n",
      "13.0 score for episode 12\n",
      "19.0 score for episode 13\n",
      "12.0 score for episode 14\n",
      "18.0 score for episode 15\n",
      "15.0 score for episode 16\n",
      "20.0 score for episode 17\n",
      "10.0 score for episode 18\n",
      "26.0 score for episode 19\n",
      "40.0 score for episode 20\n",
      "16.0 score for episode 21\n",
      "21.0 score for episode 22\n",
      "25.0 score for episode 23\n",
      "31.0 score for episode 24\n",
      "10.0 score for episode 25\n",
      "33.0 score for episode 26\n",
      "10.0 score for episode 27\n",
      "28.0 score for episode 28\n",
      "10.0 score for episode 29\n",
      "25.0 score for episode 30\n",
      "39.0 score for episode 31\n",
      "17.0 score for episode 32\n",
      "50.0 score for episode 33\n",
      "9.0 score for episode 34\n",
      "20.0 score for episode 35\n",
      "13.0 score for episode 36\n",
      "18.0 score for episode 37\n",
      "23.0 score for episode 38\n",
      "60.0 score for episode 39\n",
      "48.0 score for episode 40\n",
      "68.0 score for episode 41\n",
      "13.0 score for episode 42\n",
      "18.0 score for episode 43\n",
      "26.0 score for episode 44\n",
      "11.0 score for episode 45\n",
      "16.0 score for episode 46\n",
      "25.0 score for episode 47\n",
      "20.0 score for episode 48\n",
      "107.0 score for episode 49\n",
      "15.0 score for episode 50\n",
      "14.0 score for episode 51\n",
      "15.0 score for episode 52\n",
      "16.0 score for episode 53\n",
      "54.0 score for episode 54\n",
      "14.0 score for episode 55\n",
      "17.0 score for episode 56\n",
      "37.0 score for episode 57\n",
      "15.0 score for episode 58\n",
      "41.0 score for episode 59\n",
      "137.0 score for episode 60\n",
      "9.0 score for episode 61\n",
      "19.0 score for episode 62\n",
      "19.0 score for episode 63\n",
      "63.0 score for episode 64\n",
      "114.0 score for episode 65\n",
      "57.0 score for episode 66\n",
      "27.0 score for episode 67\n",
      "118.0 score for episode 68\n",
      "82.0 score for episode 69\n",
      "30.0 score for episode 70\n",
      "17.0 score for episode 71\n",
      "86.0 score for episode 72\n",
      "20.0 score for episode 73\n",
      "13.0 score for episode 74\n",
      "38.0 score for episode 75\n",
      "65.0 score for episode 76\n",
      "20.0 score for episode 77\n",
      "10.0 score for episode 78\n",
      "24.0 score for episode 79\n",
      "58.0 score for episode 80\n",
      "117.0 score for episode 81\n",
      "51.0 score for episode 82\n",
      "24.0 score for episode 83\n",
      "32.0 score for episode 84\n",
      "34.0 score for episode 85\n",
      "20.0 score for episode 86\n",
      "31.0 score for episode 87\n",
      "52.0 score for episode 88\n",
      "16.0 score for episode 89\n",
      "21.0 score for episode 90\n",
      "55.0 score for episode 91\n",
      "11.0 score for episode 92\n",
      "114.0 score for episode 93\n",
      "48.0 score for episode 94\n",
      "24.0 score for episode 95\n",
      "200.0 score for episode 96\n",
      "72.0 score for episode 97\n",
      "32.0 score for episode 98\n",
      "67.0 score for episode 99\n",
      "111.0 score for episode 100\n",
      "82.0 score for episode 101\n",
      "31.0 score for episode 102\n",
      "67.0 score for episode 103\n",
      "73.0 score for episode 104\n",
      "125.0 score for episode 105\n",
      "27.0 score for episode 106\n",
      "73.0 score for episode 107\n",
      "28.0 score for episode 108\n",
      "89.0 score for episode 109\n",
      "68.0 score for episode 110\n",
      "17.0 score for episode 111\n",
      "91.0 score for episode 112\n",
      "16.0 score for episode 113\n",
      "9.0 score for episode 114\n",
      "13.0 score for episode 115\n",
      "17.0 score for episode 116\n",
      "18.0 score for episode 117\n",
      "200.0 score for episode 118\n",
      "26.0 score for episode 119\n",
      "71.0 score for episode 120\n",
      "18.0 score for episode 121\n",
      "156.0 score for episode 122\n",
      "200.0 score for episode 123\n",
      "109.0 score for episode 124\n",
      "137.0 score for episode 125\n",
      "140.0 score for episode 126\n",
      "139.0 score for episode 127\n",
      "16.0 score for episode 128\n",
      "97.0 score for episode 129\n",
      "24.0 score for episode 130\n",
      "38.0 score for episode 131\n",
      "110.0 score for episode 132\n",
      "101.0 score for episode 133\n",
      "47.0 score for episode 134\n",
      "45.0 score for episode 135\n",
      "82.0 score for episode 136\n",
      "10.0 score for episode 137\n",
      "99.0 score for episode 138\n",
      "21.0 score for episode 139\n",
      "39.0 score for episode 140\n",
      "98.0 score for episode 141\n",
      "57.0 score for episode 142\n",
      "46.0 score for episode 143\n",
      "107.0 score for episode 144\n",
      "53.0 score for episode 145\n",
      "146.0 score for episode 146\n",
      "70.0 score for episode 147\n",
      "83.0 score for episode 148\n",
      "52.0 score for episode 149\n",
      "42.0 score for episode 150\n",
      "179.0 score for episode 151\n",
      "99.0 score for episode 152\n",
      "156.0 score for episode 153\n",
      "172.0 score for episode 154\n",
      "29.0 score for episode 155\n",
      "69.0 score for episode 156\n",
      "87.0 score for episode 157\n",
      "200.0 score for episode 158\n",
      "115.0 score for episode 159\n",
      "200.0 score for episode 160\n",
      "63.0 score for episode 161\n",
      "51.0 score for episode 162\n",
      "106.0 score for episode 163\n",
      "94.0 score for episode 164\n",
      "200.0 score for episode 165\n",
      "200.0 score for episode 166\n",
      "200.0 score for episode 167\n",
      "200.0 score for episode 168\n",
      "155.0 score for episode 169\n",
      "200.0 score for episode 170\n",
      "200.0 score for episode 171\n",
      "178.0 score for episode 172\n",
      "73.0 score for episode 173\n",
      "179.0 score for episode 174\n",
      "125.0 score for episode 175\n",
      "65.0 score for episode 176\n",
      "153.0 score for episode 177\n",
      "102.0 score for episode 178\n",
      "23.0 score for episode 179\n",
      "26.0 score for episode 180\n",
      "65.0 score for episode 181\n",
      "200.0 score for episode 182\n",
      "200.0 score for episode 183\n",
      "200.0 score for episode 184\n",
      "192.0 score for episode 185\n",
      "126.0 score for episode 186\n",
      "101.0 score for episode 187\n",
      "200.0 score for episode 188\n",
      "132.0 score for episode 189\n",
      "157.0 score for episode 190\n",
      "144.0 score for episode 191\n",
      "200.0 score for episode 192\n",
      "200.0 score for episode 193\n",
      "200.0 score for episode 194\n",
      "27.0 score for episode 195\n",
      "200.0 score for episode 196\n",
      "200.0 score for episode 197\n",
      "200.0 score for episode 198\n",
      "200.0 score for episode 199\n",
      "200.0 score for episode 200\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "agent = CartPoleQAgent()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 200.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
